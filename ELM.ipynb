{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Learning Machine Tutorial\n",
    "\n",
    "Sample code just for quickly teaching how to build an ELM network using Numpy and using a quick computation algorithm.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This machine learning algorithm was designed by Huang et al. [1], using a single layer *feed forward* network.\n",
    "One of their main features is that both weights $\\mathbf{W}$ and biases $\\mathbf{b}$ are random and they don't require fitting. Thus, their output weights are independent.\n",
    "The output $\\beta$ is obtained using the following operation\n",
    "$$ \\begin{align}\n",
    "H\\beta &= T \\\\\n",
    "\\beta &= H^{\\dagger} T \\\\\n",
    "H^{\\dagger} &= \\left(H^{T} H \\right)^{-1} H^{T}\\\\\n",
    "\\end{align}$$\n",
    "WHere $H^{\\dagger}$ is the **_Moore-Penrose Pseudoinverse_**, and $H$ is a full-rank column matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moore-Penrose PseudoInverse\n",
    "This is a generalization form of the inverse matrix, which allows solving least-squared systems which don't have a well-defined solution.\n",
    "\n",
    "A matrix $G$ of size $m\\times n$ has a Pseudoinverse $G^{\\dagger}$ of size $n\\times m$ which has the following properties:\n",
    "\n",
    "$$G G^{\\dagger} G = G$$\n",
    "$$G^{\\dagger} G G^{\\dagger} = G^{\\dagger}$$\n",
    "$$\\left( G G^{\\dagger} \\right)' = G G^{\\dagger}$$\n",
    "$$\\left(G^{\\dagger} G \\right)' = G^{\\dagger} G$$\n",
    "\n",
    "This calculation is computationally complex. Normally, the least-squares method or Eigen-value Decomposition are used (most commonly on `MATLAB` and `SciPy`).\n",
    "\n",
    "Courrieu [2] proposed a more computationally simple algorithm, based on reverse ordering and Cholesky factorization of a symmetric full-rank positive matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining Moore-Penrose PseudoInverse as defined by Courrieu (original in Matlab)\n",
    "def mp_pinv(G):\n",
    "    m,n = G.shape #Shape of matrix\n",
    "    transpose = False\n",
    "    \n",
    "    if m < n:\n",
    "        transpose = True #If m is bigger we multiply it with its transpose\n",
    "        A = np.matmul(G,G.T) \n",
    "        n = m\n",
    "    else:\n",
    "        A = np.matmul(G.T,G) #Else, it's backwards\n",
    "    \n",
    "    #Full-rank Choilesky\n",
    "    L = np.linalg.cholesky(A) \n",
    "    \n",
    "    #This part is doing the same without the NumPy function, I left it here just if you want to test it.\n",
    "    #dA = np.diag(A)\n",
    "    #tol = np.min(dA[dA>0])*1e-9\n",
    "    #L = np.zeros_like(A,dtype=np.float) #Zeros-like matrix like A\n",
    "    #r = 0\n",
    "    #for k in range(n):\n",
    "    #    L[k:n,r] = A[k:n,k]-np.matmul(L[k:n,0:(r)],L[k,0:(r)].T)\n",
    "    #    #If is higher than tolerance value:\n",
    "    #    if L[k,r] > tol:\n",
    "    #        L[k,r]=np.sqrt(L[k,r]) #We get square-root\n",
    "    #        if k<n:\n",
    "    #            L[(k+1):n,r]= L[(k+1):n,r]/L[k,r]; #And divide it\n",
    "    #    else:\n",
    "    #        r-=1;\n",
    "    #    r+=1 \n",
    "    #L=L[:,0:r]\n",
    "    \n",
    "    #We compute generalized inverse\n",
    "    M = np.linalg.inv(np.matmul(L.T,L))\n",
    "    if transpose:\n",
    "        #Y = G.T * L * M * M * L.T\n",
    "        Y = np.matmul(np.matmul(np.matmul(np.matmul(G.T,L),M),M),L.T)\n",
    "    else:\n",
    "        #Y = L* M * M *L.T * G.T\n",
    "        Y = np.matmul(np.matmul(np.matmul(np.matmul(L,M),M),L.T),G.T)\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing this method with NumPy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo SVD numpy: 0.719 ms\n",
      "\n",
      "Courrieu method: 1.379 ms\n",
      "\n",
      "Mean Squared Error between these two: 1.1021139939204808e-28\n"
     ]
    }
   ],
   "source": [
    "matrix = np.random.rand(800).reshape(25,32) #Random Matrix as input\n",
    "\n",
    "t0 = time()\n",
    "m01 =np.linalg.pinv(matrix)\n",
    "print(\"Metodo SVD numpy: {:.3f} ms\".format((time()-t0)*1000))\n",
    "\n",
    "t0 = time()\n",
    "m02 = mp_pinv(matrix)\n",
    "print(\"\\nCourrieu method: {:.3f} ms\".format((time()-t0)*1000))\n",
    "print(\"\\nMean Squared Error between these two: {}\".format(np.mean((m01-m02)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*_ TODO: compile using numba and do a better comparison..._*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Breast Cancer\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,Y = load_breast_cancer(return_X_y=True)\n",
    "X = (X- X.min(axis=0))/(X.max(axis=0)-X.min(axis=0)) #Normalization between 0 - 1\n",
    "\n",
    "#Splitting Train and test Data\n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,Y,train_size = 0.7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Params\n",
    "n_train = len(Xtrain)\n",
    "n_test = len(Xtest)\n",
    "n_data = Xtrain.shape[-1] #Number of samples\n",
    "n_hidden = 1000 #Hidden neurons\n",
    "\n",
    "#Training\n",
    "true = Ytrain.reshape(n_train,1).T #Outputs\n",
    "w_i = np.random.rand(n_hidden,n_data)*2 -1 #Input weights\n",
    "b_i = np.random.rand(n_hidden,1) #Input biases\n",
    "\n",
    "b_m = b_i * np.ones(n_train) #Bias matrix for all values\n",
    "temp_H = np.matmul(w_i,Xtrain.T)+b_m # wX+b\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "H = sigmoid(temp_H) #Activation function\n",
    "# Computing Beta with the Pseudoinverse\n",
    "H_cross = mp_pinv(H.T)\n",
    "beta_i = np.matmul(H_cross,true.T) #And multiply it with the output vector\n",
    "Y = np.matmul(H.T,beta_i) #And our training prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.96%\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "b_mt = b_i *np.ones(n_test) \n",
    "temp_Htest = np.matmul(w_i,Xtest.T) + b_mt # wX + b\n",
    "H_test = sigmoid(temp_Htest) \n",
    "testY = np.matmul(H_test.T,beta_i) # Prediction\n",
    "\n",
    "Ypred = np.where(testY <= 0.5,0,1) #If it's higher than 0.5, then is positive, else not.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test Accuracy: {:.2f}%\".format(accuracy_score(Ytest,Ypred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "* I plan to do a Tensorflow or PyTorch implementation of this same code. If someone wants to use it\n",
    "* Compiling in order to better compare Courrieu's method, specially without using Cholesky from numpy...\n",
    "\n",
    "This code is way far from optimal, but can be a simple example just for fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Huang, G. B., Zhu, Q. Y., & Siew, C. K. (2004, July). Extreme learning machine: a new learning scheme of feedforward neural networks. In Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on (Vol. 2, pp. 985-990). IEEE.\n",
    "2. Courrieu, P. (2008). Fast computation of Moore-Penrose inverse matrices. arXiv preprint arXiv:0804.4809. Obtained from https://arxiv.org/abs/0804.4809\n",
    "3. Akusok, A., Björk, K. M., Miche, Y., & Lendasse, A. (2015). High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications. IEEE Access, 3, 1011–1025. https://doi.org/10.1109/ACCESS.2015.2450498\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
